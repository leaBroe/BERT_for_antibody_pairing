{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-28T12:30:37.933426Z"},"trusted":true},"outputs":[],"source":["# !pip install git+https://github.com/adapter-hub/adapters.git\n","# !pip install wandb\n","# !pip install pandas\n","# !pip install datasets\n","\n","# requires ipykernel package"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !pip install accelerate -U"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["from transformers import BertGenerationEncoder, BertGenerationDecoder, EncoderDecoderModel, Seq2SeqTrainingArguments, BertTokenizer, Seq2SeqTrainer, AutoModel, AutoModelForCausalLM, DataCollatorForSeq2Seq, GenerationConfig, DataCollatorWithPadding\n","from adapters import BnConfig, Seq2SeqAdapterTrainer, AdapterTrainer, BertAdapterModel, init\n","import wandb\n","import torch\n","import pandas as pd\n","from datasets import Dataset\n","import os\n","import datasets\n","import numpy as np\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# print device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"device: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"Exscientia/IgBert\", \"Exscientia/IgBert\")\n","init(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["config = BnConfig(mh_adapter=True, output_adapter=True, reduction_factor=16, non_linearity=\"relu\")\n","\n","model.add_adapter(\"seq2seq_adapter\", config=config)\n","model.set_active_adapters(\"seq2seq_adapter\")\n","model.train_adapter(\"seq2seq_adapter\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#print(f\"print EncoderDecoderModel: {model}\")\n","\n","# Load the tokenizer and model from Hugging Face\n","tokenizer = BertTokenizer.from_pretrained(\"Exscientia/IgBert\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["generation_config = GenerationConfig(\n","    num_return_sequences=1,\n","    max_length=512,\n","    min_length=50,\n","    early_stopping = True,\n","    \n","    length_penalty = -2.0,\n","    \n","    num_beams = 4,\n","\n","    # sampling\n","    do_sample=True,\n","    top_k=50,\n","    \n","    no_repeat_ngram_size = 3,\n","\n","    # distribution adjustment\n","    temperature=0.001,\n","    repetition_penalty=1,\n","\n","    vocab_size=model.config.encoder.vocab_size,\n","\n","    # token ids\n","    pad_token_id=tokenizer.pad_token_id,\n","    eos_token_id=tokenizer.sep_token_id,\n","    decoder_start_token_id=tokenizer.cls_token_id,\n","\n","    # others\n","    use_cache=True,\n","    output_logits=True,\n","    output_scores=True,\n","    output_hidden_states=True,\n","    return_dict_in_generate=True, )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["generation_config.save_pretrained(\"generation_config\", \"generation_config_1.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["generation_config_name = \"generation_config_1\"\n","generation_config = GenerationConfig.from_pretrained(\"generation_config\", f\"{generation_config_name}.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["batch_size = 8\n","num_train_epochs = 5\n","learning_rate = 1e-4\n","\n","\n","# Set up the run name\n","run_name=f\"new_small_data_with_adapters_batch_size_{batch_size}_epochs_{num_train_epochs}_automodel_lr_{learning_rate}_{generation_config_name}\"\n","\n","output_dir = f\"./{run_name}\"\n","logging_dir = f\"./{run_name}_logging\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["training_args = Seq2SeqTrainingArguments(\n","    output_dir=output_dir,\n","    logging_dir=logging_dir,\n","    evaluation_strategy=\"steps\",\n","    logging_strategy=\"steps\",\n","    logging_steps=10,\n","    learning_rate=learning_rate,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=num_train_epochs,\n","    predict_with_generate=True,\n","    report_to=\"wandb\",\n","    run_name=run_name,\n","    generation_config=generation_config,\n",")\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","\n","# Create directories if they do not exist\n","os.makedirs(training_args.output_dir, exist_ok=True)\n","os.makedirs(training_args.logging_dir, exist_ok=True)\n","\n","# Log in to Weights & Biases\n","#wandb.login()\n","\n","\n","wandb.init(project=\"bert2bert-translation\", name=run_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def load_data(file_path):\n","    data = []\n","    with open(file_path, 'r') as file:\n","        for line in file:\n","            data.append(line.strip())\n","\n","    sequences = []\n","    for entry in data:\n","        split_entry = entry.split(' [SEP] ')\n","        if (len(split_entry) == 2):\n","            sequences.append(split_entry)\n","        else:\n","            print(f\"Skipping invalid entry: {entry}\")\n","\n","    df = pd.DataFrame(sequences, columns=['heavy', 'light'])\n","    return df\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip show datasets\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load training and validation data\n","\n","train_file_path = '/ibmm_data2/oas_database/paired_lea_tmp/paired_model/BERT2BERT/data/paired_full_seqs_sep_train_no_ids_small_SPACE_separated.txt'\n","val_file_path = '/ibmm_data2/oas_database/paired_lea_tmp/paired_model/BERT2BERT/data/paired_full_seqs_sep_val_no_ids_small_SPACE_separated.txt'\n","#test_file_path = '/ibmm_data2/oas_database/paired_lea_tmp/paired_model/train_test_val_datasets/heavy_sep_light_seq/paired_full_seqs_sep_test_no_ids_space_separated_SMALL.txt'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_df = load_data(train_file_path)\n","val_df = load_data(val_file_path)\n","#test_df = load_data(test_file_path)\n","\n","\n","encoder_max_length = 200\n","decoder_max_length = 200\n","\n","def process_data_to_model_inputs(batch):\n","    # tokenize the inputs and labels\n","    inputs = tokenizer(batch[\"light\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n","    outputs = tokenizer(batch[\"heavy\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n","\n","    batch[\"input_ids\"] = inputs.input_ids\n","    batch[\"attention_mask\"] = inputs.attention_mask\n","    #batch[\"decoder_input_ids\"] = outputs.input_ids\n","    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n","    batch[\"labels\"] = outputs.input_ids.copy()\n","\n","    # Ignore PAD token in the labels\n","    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n","\n","    return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Convert the dataframes to Hugging Face datasets\n","train_dataset = Dataset.from_pandas(train_df[['heavy', 'light']])\n","val_dataset = Dataset.from_pandas(val_df[['heavy', 'light']])\n","#test_dataset = Dataset.from_pandas(test_df[['heavy', 'light']])\n","\n","\n","train_data = train_dataset.map(\n","    process_data_to_model_inputs,\n","    batched=True,\n","    batch_size=batch_size,\n",")\n","\n","# \"decoder_input_ids\",\n","train_data.set_format(\n","    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_attention_mask\", \"labels\"],\n",")\n","\n","val_data = val_dataset.map(\n","    process_data_to_model_inputs,\n","    batched=True,\n","    batch_size=batch_size,\n",")\n","\n","# \"decoder_input_ids\",\n","val_data.set_format(\n","    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_attention_mask\", \"labels\"],\n",")\n","\n","\n","# test_data = test_dataset.map(\n","#     process_data_to_model_inputs,   \n","#     batched=True,\n","#     batch_size=batch_size,\n","# )   \n","\n","# # \"decoder_input_ids\",\n","# test_data.set_format(\n","#     type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_attention_mask\", \"labels\"],\n","# )\n","\n","\n","\n","\n","# print heavy and light seq from the first example in the training data (train_dataset)\n","print(f\"first example heavy and light seq {train_dataset[0]}, {train_dataset[1]}\")\n","\n","\n","# Initialize the trainer\n","trainer = Seq2SeqAdapterTrainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    args=training_args,\n","    train_dataset=train_data,\n","    eval_dataset=val_data,\n","    data_collator=data_collator,\n","    adapter_names=[\"seq2seq_adapter\"],\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.generation_config"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.config.decoder_start_token_id = tokenizer.cls_token_id\n","model.config.eos_token_id = tokenizer.sep_token_id\n","model.config.pad_token_id = tokenizer.pad_token_id"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#print(f\"trainer.get_train_dataloader().collate_fn: {trainer.get_train_dataloader().collate_fn}\")\n","\n","# Train the model\n","trainer.train()\n","#trainer.evaluate()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["input_prompt = \"S T G V A F M E I N G L R S D D T A T Y F C A I N R V G D R G S N P S Y F Q D W G Q G T R V T V S S \"\n","print(f\"input_prompt: {input_prompt}\")\n","\n","inputs = tokenizer(input_prompt, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n","input_ids = inputs.input_ids.to(device)\n","attention_mask = inputs.attention_mask.to(device)\n","\n","print(f\"attention_mask: {attention_mask}\")\n","\n","#input_ids = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(device)\n","print(f\"input_ids: {input_ids}\")\n","\n","# Generate text using the model\n","generated_seq = model.generate(input_ids=input_ids, \n","                               attention_mask=attention_mask, \n","                               max_length=100, \n","                               output_scores=True, \n","                               return_dict_in_generate=True)\n","\n","# Turn output scores to probabilities\n","# generated_seq_probs = torch.nn.functional.softmax(generated_seq['scores'][0], dim=-1)\n","\n","# Access the first element in the generated sequence\n","sequence = generated_seq[\"sequences\"][0]\n","\n","# Print the generated sequences and probabilities\n","print(f\"encoded heavy sequence: {sequence}.\")\n","\n","# Convert the generated IDs back to text\n","generated_text = tokenizer.decode(sequence, skip_special_tokens=True)\n","\n","print(\"decoded heavy sequence: \", generated_text)\n","\n","# print(test_data)\n","\n","# Load your test data\n","test_file_path = '/kaggle/input/test-file/paired_full_seqs_sep_test_no_ids_space_separated_SMALL.txt'\n","test_df = load_data(test_file_path)\n","\n","\n","# extract the light sequences from test_df\n","light_sequences = test_df[\"light\"]\n","\n","print(\"light_sequences: \", light_sequences)\n","print(f\"length of light sequences {len(light_sequences)}\")\n","\n","generated_heavy_seqs = []\n","\n","# Iterate through each sequence in the test dataset\n","for i in range(50):\n","    inputs = tokenizer(light_sequences[i], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n","    input_ids = inputs.input_ids.to(device)\n","    attention_mask = inputs.attention_mask.to(device)\n","\n","    generated_seq = model.generate(input_ids=input_ids, \n","                               attention_mask=attention_mask, \n","                               max_length=100, \n","                               output_scores=True, \n","                               return_dict_in_generate=True,\n","                                   generation_config=generation_config)\n","    \n","    # Access the first element in the generated sequence\n","    sequence = generated_seq[\"sequences\"][0]\n","\n","    # Print the generated sequences and probabilities\n","    print(f\"encoded heavy sequence: {sequence}.\")\n","\n","    # Convert the generated IDs back to text\n","    generated_text = tokenizer.decode(sequence, skip_special_tokens=True)\n","\n","    print(\"decoded heavy sequence: \", generated_text)\n","\n","    generated_heavy_seqs.append(generated_text)\n","\n","\n","print(\"generated_heavy_seqs:\")\n","# print each generated sequence on new line\n","for seq in generated_heavy_seqs:\n","    print(seq)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5122894,"sourceId":8568345,"sourceType":"datasetVersion"},{"datasetId":5122911,"sourceId":8568364,"sourceType":"datasetVersion"},{"datasetId":5294232,"sourceId":8803254,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":4}
