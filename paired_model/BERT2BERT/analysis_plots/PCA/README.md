### Principal Component Analysis (PCA) for CNN Embeddings

Principal Component Analysis (PCA) is a dimensionality reduction technique that helps in transforming high-dimensional data into a lower-dimensional form while preserving as much variance as possible. Here, I applied it to embeddings generated by an BERT EncoderDecoder model (heavy2light model).

### What PCA does

1. **Dimensionality Reduction:** PCA reduces the number of dimensions by transforming the data into a new set of orthogonal (uncorrelated) axes called principal components. These components are ordered by the amount of variance they capture from the data.

2. **Linear Relationships:** PCA assumes that the data can be well-represented in a lower-dimensional **linear** subspace. This means it looks for linear combinations of the original features that capture the most variance.

### How PCA Works

1. **Standardization:** If the features have different scales, standardize the data so that each feature has a mean of 0 and a standard deviation of 1.

2. **Covariance Matrix Computation:** Compute the covariance matrix of the standardized data.

3. **Eigen Decomposition:** Calculate the eigenvalues and eigenvectors of the covariance matrix.

4. **Principal Components:** Sort the eigenvalues and their corresponding eigenvectors in descending order. **The top eigenvalues and their eigenvectors form the principal components.**

5. **Projection:** Project the original data onto the principal components to obtain the reduced-dimensional data.

### PCA for CNN Embeddings

CNN embeddings are high-dimensional vectors that capture the features of the input data. Applying PCA to these embeddings can help reduce their dimensionality for e.g visualization.

### Assumption of Linear Relationships

PCA assumes that the important structure in the data can be captured through linear relationships between features. This means PCA looks for directions (principal components) in which the data varies the most, and these directions are linear combinations of the original features.

#### Implications

- **Effective for Linearly Separable Data:** PCA works well if the data structure is linear or approximately linear. It captures the maximum variance in the first few components, which can then be used to understand the data's structure.
- **Limitations for Non-Linear Data:** If the data has complex, non-linear relationships, PCA might not capture the underlying structure effectively. In this example here, the differentiation between the two groups lambda and kappa works quite well, but the smaller subgroups within the kappa and lambda groups show an overlap in the plot. 

### Interpretation of Overlap in PCA Plots

1. **Intrinsic Overlap:** The original high-dimensional data may naturally have overlapping clusters. PCA reflects this overlap in the lower-dimensional space.
2. **Variance Explanation:** PCA prioritizes directions with the highest variance. If the primary components capture significant but overlapping variance, it indicates that the clusters are not perfectly separable based on **linear** relationships.
3. **Dimensionality Reduction Loss:** Some information is inevitably lost during dimensionality reduction. Overlap can sometimes be a result of projecting high-dimensional data into a very low-dimensional space (e.g., 2D), where it's impossible to separate all clusters perfectly.