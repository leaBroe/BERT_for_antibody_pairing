# Encoder-Decoder Models for Antibody Pairing using IgBERT or Custom Heavy/Light Models

This directory contains scripts for training Encoder-Decoder models designed for antibody pairing tasks. The models can be trained either with or without adapters, utilizing IgBERT from [Exscientia](https://huggingface.co/Exscientia/IgBert) or our custom-built models. Specifically, the [heavy model](https://github.com/leaBroe/BERT_for_antibody_pairing/tree/master/heavy_model) serves as the encoder, while the [light model](https://github.com/leaBroe/BERT_for_antibody_pairing/tree/master/light_model) functions as the decoder.

## Contents

- **[Data](https://github.com/leaBroe/BERT_for_antibody_pairing/tree/master/paired_model/BERT2BERT/data):**  
  This folder includes example input data formatted as "heavy sequence [SEP] light sequence". The data is structured to facilitate the training and evaluation of the Encoder-Decoder models in generating light chain sequences given a heavy chain sequence, or vice versa.

- **[Tutorial](https://github.com/leaBroe/BERT_for_antibody_pairing/tree/master/paired_model/BERT2BERT/tutorial):**  
  This tutorial provides a step-by-step guide on how to implement a BERT2BERT model, adapted from [this Colab notebook](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Leveraging_Pre_trained_Checkpoints_for_Encoder_Decoder_Models.ipynb#scrollTo=mIBy7uK3Od4B). The tutorial introduces the concept of leveraging pre-trained checkpoints for sequence generation tasks, as first described in the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461).

- **alphafold:**  
  Contains a Jupyter notebook for running AlphaFold predictions on a single sequence. This notebook is based on ColabFold v1.5.5, which integrates AlphaFold2 and Alphafold2-multimer for protein structure and complex prediction, using sequence alignments/templates generated by MMseqs2 and HHsearch. The notebook includes links to the ColabFold GitHub repository and a detailed manuscript for further reference.

- **analysis_plot:**  
  This folder holds various plots, including UMAPs, t-SNE, and PCA visualizations, comparing the performance of the custom heavy2light model and IgBERT2IgBERT model.

- **logs:**  
  Contains output files from full model evaluations, including metrics such as perplexity, similarity, and BLOSUM score, providing detailed insights into model performance.

- **sqlite3_data_for_analysis:**  
  Includes detailed analyses of model performance, such as kappa lambda light chain analysis, B-cell analysis, and evaluation of the test set by regions (e.g., FWR regions 1-4 and CDR regions 1-3) based on methodologies from the IgBERT paper.

## How to Use

1. **Training the Model:**  
   Use the scripts in this directory to train your Encoder-Decoder model. You can choose to train with or without adapters, depending on your specific requirements. The model architecture supports both the pre-trained IgBERT from Exscientia and custom heavy/light models developed in-house.

2. **Data Preparation:**  
   Ensure your input data follows the format specified in the [data folder](https://github.com/leaBroe/BERT_for_antibody_pairing/tree/master/paired_model/BERT2BERT/data). Each data entry should contain a heavy chain sequence and a light chain sequence, separated by a `[SEP]` token.

3. **Running the Tutorial:**  
   Follow the detailed tutorial linked above to get started with the BERT2BERT model. The tutorial walks through setting up the environment, preparing data, training the model, and evaluating its performance.

4. **AlphaFold Predictions:**  
   Utilize the notebook in the **alphafold** folder to predict protein structures using AlphaFold. This is particularly useful for analyzing the structure of the generated antibody sequences.
